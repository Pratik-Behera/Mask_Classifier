{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_directory='C:\\\\Users\\\\tamyl\\\\OneDrive\\\\Desktop\\\\files\\\\MP_VERZEO'\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "##importing for model creation\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense,Dropout,Flatten\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers.convolutional import Conv2D,MaxPooling2D\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import tensorflow "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE=100\n",
    "dataset=[]\n",
    "categories=['with_mask','without_mask']#with_mask=0,without_mask=1\n",
    "for category in categories:\n",
    "    path=os.path.join(data_directory,category)\n",
    "    class_num=categories.index(category)\n",
    "    for img in os.listdir(path):\n",
    "        img_array=cv2.imread(os.path.join(path,img))\n",
    "        new_array=cv2.resize(img_array,(IMG_SIZE,IMG_SIZE))\n",
    "        g_new_array=cv2.cvtColor(new_array,cv2.COLOR_BGR2GRAY)#gray_scale_array\n",
    "        e_new_array=cv2.equalizeHist(g_new_array)#equalizeHist_array\n",
    "        u_new_array=e_new_array/255#to ensure all values are between 0 ansd 1\n",
    "        dataset.append([u_new_array,class_num])\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.shuffle(dataset)# for better training of the model(learning without confusion)\n",
    "X=[]\n",
    "y=[]\n",
    "for features,labels in dataset:\n",
    "    X.append(features)\n",
    "    y.append(labels)\n",
    "X=np.array(X)\n",
    "y=np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_validation,y_train,y_validation=train_test_split(X_train,y_train,test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=X_train.reshape(X_train.shape[0],X_train.shape[1],X_train.shape[2],1)\n",
    "X_test=X_test.reshape(X_test.shape[0],X_test.shape[1],X_test.shape[2],1)\n",
    "X_validation=X_validation.reshape(X_validation.shape[0],X_validation.shape[1],X_validation.shape[2],1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(276, 100, 100, 1) (880, 100, 100, 1) (220, 100, 100, 1)\n"
     ]
    }
   ],
   "source": [
    "a=X_test.shape\n",
    "b=X_train.shape\n",
    "c=X_validation.shape\n",
    "print(a,b,c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train=to_categorical(y_train)#  onr-not enconding on the datasets\n",
    "y_test=to_categorical(y_test)\n",
    "y_validation=to_categorical(y_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(880, 2) (276, 2) (220, 2)\n"
     ]
    }
   ],
   "source": [
    "d=y_train.shape\n",
    "e=y_test.shape\n",
    "f=y_validation.shape\n",
    "print(d,e,f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataGen=ImageDataGenerator(rotation_range=10,width_shift_range=0.1,height_shift_range=0.1,zoom_range=0.1,shear_range=0.1)\n",
    "dataGen.fit(X_train)\n",
    "batches=dataGen.flow(X_train,y_train,batch_size=32)#process 20 images at a time\n",
    "X_batch,y_batch=next(batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Conv2D(70,(5,5),activation='relu',input_shape=(100,100,1)))\n",
    "model.add(Conv2D(70,(5,5),activation='relu'))\n",
    "model.add(MaxPooling2D(2,2))\n",
    "model.add(Conv2D(70//2,(3,3),activation='relu'))\n",
    "model.add(Conv2D(70//2,(3,3),activation='relu'))\n",
    "model.add(MaxPooling2D(2,2))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(500,activation='relu'))\n",
    "#model.add(Dropout(0.5))\n",
    "model.add(Dense(2,activation='softmax'))#output layer\n",
    "model.compile(Adam(lr=0.001),loss='binary_crossentropy',metrics='accuracy')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "28/28 [==============================] - 79s 3s/step - loss: 0.6971 - accuracy: 0.5270 - val_loss: 0.6401 - val_accuracy: 0.7045\n",
      "Epoch 2/25\n",
      "28/28 [==============================] - 78s 3s/step - loss: 0.6315 - accuracy: 0.6751 - val_loss: 0.5357 - val_accuracy: 0.7636\n",
      "Epoch 3/25\n",
      "28/28 [==============================] - 78s 3s/step - loss: 0.5648 - accuracy: 0.7285 - val_loss: 0.5805 - val_accuracy: 0.6818\n",
      "Epoch 4/25\n",
      "28/28 [==============================] - 78s 3s/step - loss: 0.5791 - accuracy: 0.7178 - val_loss: 0.4496 - val_accuracy: 0.7727\n",
      "Epoch 5/25\n",
      "28/28 [==============================] - 78s 3s/step - loss: 0.4799 - accuracy: 0.7733 - val_loss: 0.4365 - val_accuracy: 0.8182\n",
      "Epoch 6/25\n",
      "28/28 [==============================] - 76s 3s/step - loss: 0.4780 - accuracy: 0.7700 - val_loss: 0.3181 - val_accuracy: 0.8364\n",
      "Epoch 7/25\n",
      "28/28 [==============================] - 73s 3s/step - loss: 0.4032 - accuracy: 0.8133 - val_loss: 0.2384 - val_accuracy: 0.8818\n",
      "Epoch 8/25\n",
      "28/28 [==============================] - 72s 3s/step - loss: 0.3156 - accuracy: 0.8540 - val_loss: 0.2441 - val_accuracy: 0.9000\n",
      "Epoch 9/25\n",
      "28/28 [==============================] - 72s 3s/step - loss: 0.3067 - accuracy: 0.8762 - val_loss: 0.2570 - val_accuracy: 0.9045\n",
      "Epoch 10/25\n",
      "28/28 [==============================] - 75s 3s/step - loss: 0.2920 - accuracy: 0.8893 - val_loss: 0.2486 - val_accuracy: 0.8773\n",
      "Epoch 11/25\n",
      "28/28 [==============================] - 78s 3s/step - loss: 0.2912 - accuracy: 0.8732 - val_loss: 0.1812 - val_accuracy: 0.9227\n",
      "Epoch 12/25\n",
      "28/28 [==============================] - 78s 3s/step - loss: 0.2466 - accuracy: 0.8941 - val_loss: 0.1528 - val_accuracy: 0.9455\n",
      "Epoch 13/25\n",
      "28/28 [==============================] - 77s 3s/step - loss: 0.2255 - accuracy: 0.9169 - val_loss: 0.1212 - val_accuracy: 0.9727\n",
      "Epoch 14/25\n",
      "28/28 [==============================] - 77s 3s/step - loss: 0.2168 - accuracy: 0.9102 - val_loss: 0.2164 - val_accuracy: 0.8955\n",
      "Epoch 15/25\n",
      "28/28 [==============================] - 78s 3s/step - loss: 0.2632 - accuracy: 0.8864 - val_loss: 0.1465 - val_accuracy: 0.9500\n",
      "Epoch 16/25\n",
      "28/28 [==============================] - 77s 3s/step - loss: 0.1867 - accuracy: 0.9156 - val_loss: 0.1271 - val_accuracy: 0.9636\n",
      "Epoch 17/25\n",
      "28/28 [==============================] - 77s 3s/step - loss: 0.1932 - accuracy: 0.9214 - val_loss: 0.1417 - val_accuracy: 0.9500\n",
      "Epoch 18/25\n",
      "28/28 [==============================] - 77s 3s/step - loss: 0.2428 - accuracy: 0.9108 - val_loss: 0.1159 - val_accuracy: 0.9682\n",
      "Epoch 19/25\n",
      "28/28 [==============================] - 78s 3s/step - loss: 0.1855 - accuracy: 0.9268 - val_loss: 0.1301 - val_accuracy: 0.9545\n",
      "Epoch 20/25\n",
      "28/28 [==============================] - 78s 3s/step - loss: 0.1927 - accuracy: 0.9190 - val_loss: 0.1352 - val_accuracy: 0.9500\n",
      "Epoch 21/25\n",
      "28/28 [==============================] - 78s 3s/step - loss: 0.1647 - accuracy: 0.9316 - val_loss: 0.1134 - val_accuracy: 0.9500\n",
      "Epoch 22/25\n",
      "28/28 [==============================] - 78s 3s/step - loss: 0.1256 - accuracy: 0.9574 - val_loss: 0.0840 - val_accuracy: 0.9591\n",
      "Epoch 23/25\n",
      "28/28 [==============================] - 78s 3s/step - loss: 0.1397 - accuracy: 0.9446 - val_loss: 0.1495 - val_accuracy: 0.9136\n",
      "Epoch 24/25\n",
      "28/28 [==============================] - 78s 3s/step - loss: 0.1447 - accuracy: 0.9307 - val_loss: 0.1111 - val_accuracy: 0.9591\n",
      "Epoch 25/25\n",
      "28/28 [==============================] - 77s 3s/step - loss: 0.1252 - accuracy: 0.9435 - val_loss: 0.1212 - val_accuracy: 0.9500\n"
     ]
    }
   ],
   "source": [
    "final=model.fit(dataGen.flow(X_train,y_train,batch_size=32),validation_data=(X_validation,y_validation),epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final=model.fit(dataGen.flow(X_train,y_train,batch_size=40),validation_data=(X_validation,y_validation),epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_clsfr=cv2.CascadeClassifier('C:\\\\Users\\\\tamyl\\\\OneDrive\\\\Desktop\\\\haarcascade_frontalface_default.xml')#location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to be edited accordingly.\n",
    "labels_dict={0:'with_mask',1:'without_mask'}\n",
    "color_dict={0:(0,255,0),1:(0,0,255)}\n",
    "size=4\n",
    "webcam=cv2.VideoCapture(0)\n",
    "#classifier=cv2.CasacadeClassifier('C:\\\\Users\\\\tamyl\\\\OneDrive\\\\Desktop\\\\haarcascade_frontalface_default.xml')\n",
    "while True:\n",
    "    (rval,im)=webcam.read()\n",
    "    im=cv2.flip(im,1,1)\n",
    "    mini=cv2.resize(im,(im.shape[1]// size,im.shape[0]//size))\n",
    "    faces=face_clsfr.detectMultiScale(mini)\n",
    "    for f in faces:\n",
    "        (x,y,w,h)=[v*size for v in f]\n",
    "        face_img = im[y:y+h, x:x+w]\n",
    "        resized=cv2.resize(face_img,(100,100))\n",
    "        normalized=resized/255.0\n",
    "        reshaped=np.reshape(normalized,(-1,100,100,1))\n",
    "        reshaped = np.vstack([reshaped])\n",
    "        result=model.predict(reshaped)\n",
    "\n",
    "        label=np.argmax(result,axis=1)[0]\n",
    "      \n",
    "        cv2.rectangle(im,(x,y),(x+w,y+h),color_dict[label],2)\n",
    "        cv2.rectangle(im,(x,y-40),(x+w,y),color_dict[label],-1)\n",
    "        cv2.putText(im, labels_dict[label], (x, y-10),cv2.FONT_HERSHEY_COMPLEX,0.8,(255,255,255),2)\n",
    "        \n",
    "\n",
    "    cv2.imshow('LIVE',   im)\n",
    "    key = cv2.waitKey(10)\n",
    "\n",
    "    if key == 27: \n",
    "        break\n",
    "\n",
    "webcam.release()\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
